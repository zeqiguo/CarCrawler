{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromeBot\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'}\n",
    "\n",
    "root = 'https://www.carcomplaints.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    '''\n",
    "    url: url that need to be crawled \n",
    "    '''\n",
    "    ################### INIT FUNCTION ###################\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "        \n",
    "    ################### PRIVATE FUNCTIONS ##################\n",
    "    def __to_html(self):\n",
    "        html = requests.get(url=self.url, headers=headers)\n",
    "        return html.text\n",
    "    \n",
    "    \n",
    "    ##################### DECORATOR ########################\n",
    "    def log(text):\n",
    "        def decorator(func):\n",
    "            def wrapper(*args, **kw):\n",
    "                print('%s %s():' % (text, func.__name__))\n",
    "                return func(*args, **kw)\n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "\n",
    "    ################### BASIC FUNCTIONS ###################\n",
    "    @log('execute')\n",
    "    def get_head(self):\n",
    "        '''\n",
    "        :return: html <head>\n",
    "        '''\n",
    "        # return html code of head\n",
    "        html = self.__to_html()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return soup.head\n",
    "\n",
    "    @log('execute')\n",
    "    def get_title(self):\n",
    "        '''\n",
    "        :return: html <title>\n",
    "        '''\n",
    "        # return the page title\n",
    "        html = self.__to_html()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return soup.title\n",
    "\n",
    "    @log('execute')\n",
    "    def get_body(self):\n",
    "        '''\n",
    "        :return: html <body>\n",
    "        '''\n",
    "        # return html code of body\n",
    "        html = self.__to_html()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return soup.body\n",
    "\n",
    "    \n",
    "    ##################### MAKE FUNCTIONS #####################\n",
    "    @log('execute')\n",
    "    def get_makes(self) -> list:\n",
    "        '''\n",
    "        :return: [Acura]\n",
    "        \n",
    "        Return the makes as a list, prepare for further crawl, the return values in the list was removed all '/'\n",
    "        find all of a tags whitch include required title but not have class_ in body\n",
    "        '''\n",
    "        \n",
    "        return [title.get('href').replace('/', '') for title in self.get_body().find_all(title=re.compile('Complaints About'), class_=False)]\n",
    "\n",
    "    @log('execute')\n",
    "    def number_of_makes(self) -> int:\n",
    "        return len(self.get_makes())\n",
    "\n",
    "    @log('execute')\n",
    "    def get_make_urls(self, save=False) -> list:\n",
    "        '''\n",
    "        :return: [https://www.carcomplaints.com/Acura]\n",
    "        \n",
    "        Use makes list from self.makes to combine with root \n",
    "        '''\n",
    "        path = 'Make/'\n",
    "        make_url_pool = []\n",
    "        makes = self.get_makes()\n",
    "\n",
    "        for make in makes:\n",
    "            make_url = root+'/'+make\n",
    "            make_url_pool.append(make_url)\n",
    "\n",
    "#             if save:\n",
    "#                 if not os.path.exists('Make'):\n",
    "#                     os.mkdir('Make')\n",
    "#                 make_path = path + make                             # specific make path\n",
    "#                 if not os.path.exists(make_path):\n",
    "#                     os.mkdir(make_path)\n",
    "        if save and not os.path.exists('makes.npy'):\n",
    "            np.save(file='makes.npy', arr=make_url_pool)\n",
    "\n",
    "        return make_url_pool\n",
    "\n",
    "    \n",
    "    ##################### MODE FUNCTIONS ######################\n",
    "    @log('execute')\n",
    "    def get_models(self, make: str) -> list:\n",
    "        '''\n",
    "        :return: [/Acura/CL/]\n",
    "        \n",
    "        find as /Make/Model/\n",
    "        '''\n",
    "        # return models as a list\n",
    "        # all make got was removed '/'\n",
    "        c1 = Crawler(root+'/'+make)\n",
    "        return [model.get('href') for model in c1.get_body().find_all(href=re.compile('/%s/' % make), title=re.compile(r'complaints'))]\n",
    "\n",
    "    @log('execute')\n",
    "    def get_model_urls(self, save=False) -> list:\n",
    "        '''\n",
    "        :return: [[]]\n",
    "        \n",
    "        get a 2D list, each dimension means models of each make\n",
    "        '''\n",
    "        model_url_pool = []                                                     # 2D list\n",
    "        make_urls = self.get_make_urls(save)\n",
    "        makes = self.get_makes()\n",
    "\n",
    "        for make in makes:\n",
    "            # get models by giving make\n",
    "            models = self.get_models(make)\n",
    "            # use to save models of this make\n",
    "            urls = []\n",
    "            for model in models:\n",
    "                model_url = root + model\n",
    "                urls.append(model_url)\n",
    "                # path for make model dictionary\n",
    "                model_path = 'Make' + model\n",
    "                if save:\n",
    "                    if not os.path.exists(model_path):\n",
    "                        os.mkdir(model_path)\n",
    "            model_url_pool.append(urls)\n",
    "\n",
    "#             if save and not os.path.exists('models.txt'):\n",
    "#                 with open('models.txt', 'a', encoding='utf8') as f:\n",
    "#                     f.write('{}\\n{}\\n\\n' .format(make, models))\n",
    "        if save and not os.path.exists('models.npy'):\n",
    "            np.save(file='models.npy', arr=model_url_pool)\n",
    "\n",
    "        return model_url_pool\n",
    "\n",
    "    ##################### YEAR FUNCTIONS ######################\n",
    "\n",
    "    @log('execute')\n",
    "    def get_years(self, url: str) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        c1 = Crawler(url)\n",
    "        return [year.get('href') for year in c1.get_body().find_all(href=re.compile(r'%s/[0-9]{4}\\/$' % url.split('/')[-2]), \n",
    "                                                                    title=re.compile(r'Problems'))]\n",
    "\n",
    "    @log('execute')\n",
    "    def get_year_urls(self, save=False) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        get a 3D list, 2nd dimension means same model, 3rd dimension means same years.\n",
    "        '''\n",
    "        year_url_pool = []\n",
    "        model_url_pool = self.get_model_urls(\n",
    "            save)                                 # [[]]\n",
    "\n",
    "        for model_urls in model_url_pool:\n",
    "            models = []\n",
    "            for url in model_urls:\n",
    "                years = self.get_years(url)\n",
    "                year_urls = [root+year for year in years]\n",
    "                models.append(year_urls)\n",
    "            year_url_pool.append(models)\n",
    "\n",
    "        if save and not os.path.exists('years.npy'):\n",
    "            np.save(file='years.npy', arr=year_url_pool)\n",
    "\n",
    "        return year_url_pool\n",
    "\n",
    "    ##################### PROBKEM(LV1) FUNCTIONS ######################\n",
    "\n",
    "    @log('execute')\n",
    "    def get_problems(self, url: str) -> list:\n",
    "        c1 = Crawler(url)\n",
    "        return [problem.find('a').get('href') for problem in c1.get_body().find_all(id=re.compile('bar\\d{1}'), class_=True)]\n",
    "\n",
    "    @log('execute')\n",
    "    def get_problem_urls(self, save=False) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        1D list, urls of each year\n",
    "        '''\n",
    "\n",
    "        problems_url_pool = []\n",
    "\n",
    "        if not os.path.exists('years.npy'):\n",
    "            self.get_year_urls(save=True)\n",
    "\n",
    "        urls = np.load('years.npy')\n",
    "        for makes in urls:\n",
    "            for models in makes:\n",
    "                for years in models:\n",
    "                    problem_href = self.get_problems(years)\n",
    "                    problems_url_pool.append(\n",
    "                        [years+problem for problem in problem_href])\n",
    "\n",
    "        if save:\n",
    "            np.save(file='problems.npy', arr=problems_url_pool)\n",
    "\n",
    "        return problems_url_pool             # [[[]]]\n",
    "\n",
    "    ##################### PROBKEM(LV2) FUNCTIONS ######################\n",
    "    @log('execute')\n",
    "    def get_sub_problems(self, url: str) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        c1 = Crawler(url)\n",
    "        return [sub_problem.find('a').get('href') for sub_problem in c1.get_body().find_all(id=re.compile('bar\\d{1}'))]\n",
    "\n",
    "    @log('execute')\n",
    "    def get_sub_problem_urls(self, save=False) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        2D list, 2nd dimension means different sub problems.\n",
    "        '''\n",
    "        sub_problem_url_pool = []\n",
    "\n",
    "        if not os.path.exists('problems.npy'):\n",
    "            self.get_problem_urls(save=True)\n",
    "        p_urls = np.load('problems.npy')\n",
    "        for urls in p_urls:\n",
    "            problems_pool = []\n",
    "            for url in urs:\n",
    "                sub_p_href = self.get_sub_problems(url)\n",
    "                problems_pool.append(sub_p_href)\n",
    "#                 for each in sub_p_href:\n",
    "#                     sub_problem_url_pool.append(root+each)\n",
    "            sub_problem_url_pool.append(problems_pool)\n",
    "\n",
    "        if save:\n",
    "            np.save(file='sub_problems.npy', arr=sub_problem_url_pool)\n",
    "\n",
    "        return sub_problem_url_pool\n",
    "\n",
    "    #################### COMMENTS FUNCTIONS ######################\n",
    "\n",
    "    @log('execute')\n",
    "    def get_comments(self, url: str) -> list:\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # return comments div and time div\n",
    "        c1 = Crawler(url)\n",
    "        results = c1.get_body().find_all(class_=re.compile(r'^comments'))\n",
    "        return results\n",
    "    \n",
    "    @log('execute')\n",
    "    def get_date(self, url: str) -> list:\n",
    "        \n",
    "        c1 = Crawler(url)\n",
    "        time = c1.get_body().find_all(class_=re.compile(r'^pdate'))\n",
    "        return time\n",
    "\n",
    "    @log('execute')\n",
    "    def csv_header(self, all_header=False):\n",
    "        '''\n",
    "        :return:\n",
    "        \n",
    "        create a hearder, Make, Model, Year, others are name of problems\n",
    "        '''\n",
    "        cars = np.load('problems.npy')\n",
    "\n",
    "        problem_set = set()\n",
    "        for car in cars:\n",
    "            for record in car:\n",
    "                problem = record.split('/')[-2]\n",
    "                problem_set.add(problem)\n",
    "        if all_header:        \n",
    "            return ['Make', 'Model', 'Year'] + list(problem_set)\n",
    "        else:\n",
    "            return list(problem_set)\n",
    "    \n",
    "    @log('execute')\n",
    "    def problem2index(self):\n",
    "        '''\n",
    "        :return: {transmission: 1}\n",
    "        \n",
    "        pair the problem to index(int)\n",
    "        '''\n",
    "        problem_dict = {}\n",
    "        headers = self.csv_header()\n",
    "        \n",
    "        for index in range(len(headers)):\n",
    "            problem_dict[headers[index]] = index+3\n",
    "        \n",
    "        return problem_dict\n",
    "        \n",
    "\n",
    "\n",
    "    @log('execute')\n",
    "    def get_all_comments(self):\n",
    "        '''\n",
    "        :return: \n",
    "        \n",
    "        save as a csv file, all 23 columns. First three col are Make, Model, Year, other 20 are different problems. \n",
    "        '''\n",
    "        print('=======================================================================================')\n",
    "        print('============================     CRAWLER START WORKING     ============================')\n",
    "        print('=======================================================================================')\n",
    "        print('============================      Wait Paitent Please      ============================')\n",
    "        print('============================               .               ============================')\n",
    "        print('============================               .               ============================')\n",
    "        print('============================               .               ============================')\n",
    "        print('============================               .               ============================')\n",
    "        print('============================               .               ============================')\n",
    "        \n",
    "        all_comments = []\n",
    "        row = []\n",
    "        \n",
    "        header = self.csv_header(all_header=True)\n",
    "        index_table = self.problem2index()\n",
    "        \n",
    "        with open('CarV4_0.csv', mode='w', encoding='utf-8') as carFile:\n",
    "            writer = csv.writer(carFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            years = np.load('sub_0.npy')\n",
    "            new_years = list(filter(lambda x: len(x)!=0, years))\n",
    "            \n",
    "            for year_ in new_years:\n",
    "                \n",
    "#               got make model year\n",
    "                sp = year_[0][0].split('/')\n",
    "                make = sp[1]\n",
    "                model = sp[2]\n",
    "                year = sp[3]\n",
    "                row = [make, model, year] + [0 for i in range(20)]\n",
    "\n",
    "                for problems in year_:\n",
    "                    comment_list =[]\n",
    "#                   got problem\n",
    "                    problem = problems[0].split('/')[4]\n",
    "\n",
    "                    for sub_p in problems:\n",
    "            \n",
    "#                       crawler got comments and save into a list\n",
    "                        comments = self.get_comments(root+sub_p)\n",
    "                        date = self.get_date(root+sub_p)\n",
    "                        comment_list += [comments[i].text + ' ' + date[i].text for i in len(comments)]\n",
    "\n",
    "#                   find row index, in order to fill data into correct col of csv  \n",
    "                    index = index_table[problem]\n",
    "                    row[index] = comment_list         # change value at index\n",
    "                writer.writerow(row)\n",
    "            \n",
    "        print('============================               DONE            ============================')\n",
    "    \n",
    "#     def get_all_date(self):\n",
    "#         print('=======================================================================================')\n",
    "#         print('============================     CRAWLER START WORKING     ============================')\n",
    "#         print('=======================================================================================')\n",
    "#         print('============================      Wait Paitent Please      ============================')\n",
    "#         print('============================               .               ============================')\n",
    "#         print('============================               .               ============================')\n",
    "#         print('============================               .               ============================')\n",
    "#         print('============================               .               ============================')\n",
    "#         print('============================               .               ============================')\n",
    "        \n",
    "#         all_date = []\n",
    "#         header = 'Date'\n",
    "#         with open('date.csv', mode='w', encoding='utf-8') as carFile:\n",
    "#             writer = csv.writer(carFile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#             writer.writerow(header)                        \n",
    "            \n",
    "#             years = np.load('sub_1.npy')\n",
    "#             new_years = list(filter(lambda x: len(x)!=0, years)) \n",
    "\n",
    "#             for year_ in new_years:\n",
    "#                 for problems in year_:\n",
    "#                     for sub_p in problems:\n",
    "#                         date = self.get_date(root+sub_p)\n",
    "#                         date_list = [each.text for each in date]\n",
    "                    \n",
    "#                     writer.write\n",
    "                    \n",
    "#         pass\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #################### TEST FUNCTIONS ######################\n",
    "    def test(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execute get_comments():\n",
      "execute get_body():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<div class=\"comments\">\n",
       "<div class=\"ad\"><p>A D V E R T I S E M E N T S</p><div id=\"div-gpt-ad-1319716927473-6\">\n",
       "<script>\n",
       "\t\tgoogletag.cmd.push(function() { googletag.display('div-gpt-ad-1319716927473-6'); });\n",
       "\t</script>\n",
       "</div>\n",
       "</div>\t\t\tI own a 2009 Audi A4 with 63,000 miles. The Audi dealer change the pistons and rings in my vehicle due to an oil consumption problem two weeks ago. On November 4, 2014 my check engine light comes on and the car is idling up and down constantly when at a stand still. The service department informed me that the intake manifold failed internally and wants to charge $935 plus tax to repair. Under second opinion I was informed that this is a part is highly unlikely to fail and should have been inspected with the piston repair work was completed. Dealer claimed that they road tested the vehicle and everything seem fine. Contacted Audi usa to see if they will help, I have been a customer of Audi for 13 years.\n",
       "\t\t\t<p class=\"userinfo\">\n",
       "\t\t\t\t- <strong>Bloomfield, CT, USA</strong>\n",
       "</p>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################## TEST CELL #############################\n",
    "c = Crawler(root)\n",
    "\n",
    "# c.get_head()\n",
    "# c.get_title()\n",
    "# c.get_body()\n",
    "\n",
    "# c.get_makes()\n",
    "# c.number_of_makes()\n",
    "# c.get_make_urls(save=True)\n",
    "\n",
    "# c.get_models('Acura')\n",
    "# u = c.get_model_urls(save=False)\n",
    "\n",
    "# c.get_years('https://www.carcomplaints.com/Audi/A4/')\n",
    "# c.get_year_urls(save=True)\n",
    "\n",
    "# c.get_problems('https://www.carcomplaints.com/Audi/A4/2006/')\n",
    "# c.get_problem_urls(save=True)\n",
    "\n",
    "# c.get_sub_problems('https://www.carcomplaints.com/Audi/Q5/2012/engine/')\n",
    "# c.get_sub_problem_urls(save=True)\n",
    "\n",
    "# c.get_comments('https://www.carcomplaints.com/Audi/A4/2009/engine/engine_and_engine_cooling.shtml')[0]\n",
    "# c.get_time('https://www.carcomplaints.com/Audi/A4/2009/engine/engine_and_engine_cooling.shtml')[0].text\n",
    "\n",
    "\n",
    "# c.csv_header()\n",
    "\n",
    "# c.problem2index()\n",
    "\n",
    "c.get_all_comments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.load('sub_problems.npy')\n",
    "\n",
    "\n",
    "# np.save(file='sub_0.npy', arr=a[:6810])\n",
    "# np.save(file='sub_1.npy', arr=a[6810:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
